{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "[LivePortrait GitHub](https://github.com/KwaiVGI/LivePortrait) <br>\n",
        "[Hugging Face Model Card](https://huggingface.co/KwaiVGI/LivePortrait/tree/main)<br>\n",
        "[Hugging Face Space](https://huggingface.co/spaces/KwaiVGI/LivePortrait)\n",
        "\n"
      ],
      "metadata": {
        "id": "lOUrxxaVsqsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install LiveProtrait and Restart Session\n",
        "base_path=\"/content\"\n",
        "%cd $base_path\n",
        "!rm -rf $base_path/LivePortrait\n",
        "!git clone https://github.com/KwaiVGI/LivePortrait.git\n",
        "%cd $base_path/LivePortrait\n",
        "!pip install -r requirements.txt\n",
        "%cd $base_path/LivePortrait/src/utils/dependencies/XPose/models/UniPose/ops\n",
        "!python setup.py build install\n",
        "\n",
        "\n",
        "from huggingface_hub import hf_hub_download, snapshot_download\n",
        "import os\n",
        "import shutil\n",
        "import platform\n",
        "def download_models():\n",
        "  model_folder=f\"{base_path}/LivePortrait/pretrained_weights\"\n",
        "  os.makedirs(model_folder,exist_ok=True)\n",
        "  model_repo = \"KwaiVGI/LivePortrait\"\n",
        "  os.makedirs(f\"{model_folder}/docs\", exist_ok=True)\n",
        "  os.makedirs(f\"{model_folder}/insightface/models/buffalo_l\", exist_ok=True)\n",
        "  os.makedirs(f\"{model_folder}/liveportrait/base_models\", exist_ok=True)\n",
        "  os.makedirs(f\"{model_folder}/liveportrait/retargeting_models\", exist_ok=True)\n",
        "  os.makedirs(f\"{model_folder}/liveportrait_animals/base_models\", exist_ok=True)\n",
        "  os.makedirs(f\"{model_folder}/liveportrait_animals/retargeting_models\", exist_ok=True)\n",
        "  snapshot_path = snapshot_download(repo_id=model_repo, local_dir=model_folder)\n",
        "  for i in [\"/README.md\",\"/docs/inference.gif\",\"/docs/showcase2.gif\",\"/.gitignore\",\"/.gitattributes\",\"/.gitkeep\"]:\n",
        "    delete_path=f\"{model_folder}{i}\"\n",
        "    if os.path.exists(delete_path):\n",
        "      os.remove(delete_path)\n",
        "    if os.path.exists(f\"{model_folder}/docs\"):\n",
        "      shutil.rmtree(f\"{model_folder}/docs\")\n",
        "download_models()\n",
        "\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "\n",
        "import time\n",
        "time.sleep(5)\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "IgW3_O5m4WuB",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Human\n",
        "base_path=\"/content\"\n",
        "%cd $base_path/LivePortrait\n",
        "\"\"\"\n",
        "The entrance of the gradio for human\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import tyro\n",
        "import subprocess\n",
        "import gradio as gr\n",
        "import os.path as osp\n",
        "from src.utils.helper import load_description\n",
        "from src.gradio_pipeline import GradioPipeline\n",
        "from src.config.crop_config import CropConfig\n",
        "from src.config.argument_config import ArgumentConfig\n",
        "from src.config.inference_config import InferenceConfig\n",
        "\n",
        "\n",
        "def partial_fields(target_class, kwargs):\n",
        "    return target_class(**{k: v for k, v in kwargs.items() if hasattr(target_class, k)})\n",
        "\n",
        "\n",
        "def fast_check_ffmpeg():\n",
        "    try:\n",
        "        subprocess.run([\"ffmpeg\", \"-version\"], capture_output=True, check=True)\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "\n",
        "# # set tyro theme\n",
        "# tyro.extras.set_accent_color(\"bright_cyan\")\n",
        "# args = tyro.cli(ArgumentConfig)\n",
        "\n",
        "\n",
        "default_args = {\n",
        "    \"server_name\": \"0.0.0.0\",\n",
        "    \"server_port\": 7860,\n",
        "    \"share\": True,\n",
        "    # Add other default values as needed\n",
        "}\n",
        "\n",
        "# Create an args object with default values\n",
        "args = ArgumentConfig(**default_args)\n",
        "\n",
        "\n",
        "\n",
        "ffmpeg_dir = os.path.join(os.getcwd(), \"ffmpeg\")\n",
        "if osp.exists(ffmpeg_dir):\n",
        "    os.environ[\"PATH\"] += (os.pathsep + ffmpeg_dir)\n",
        "\n",
        "if not fast_check_ffmpeg():\n",
        "    raise ImportError(\n",
        "        \"FFmpeg is not installed. Please install FFmpeg (including ffmpeg and ffprobe) before running this script. https://ffmpeg.org/download.html\"\n",
        "    )\n",
        "# specify configs for inference\n",
        "inference_cfg = partial_fields(InferenceConfig, args.__dict__)  # use attribute of args to initial InferenceConfig\n",
        "crop_cfg = partial_fields(CropConfig, args.__dict__)  # use attribute of args to initial CropConfig\n",
        "# global_tab_selection = None\n",
        "\n",
        "gradio_pipeline = GradioPipeline(\n",
        "    inference_cfg=inference_cfg,\n",
        "    crop_cfg=crop_cfg,\n",
        "    args=args\n",
        ")\n",
        "\n",
        "if args.gradio_temp_dir not in (None, ''):\n",
        "    os.environ[\"GRADIO_TEMP_DIR\"] = args.gradio_temp_dir\n",
        "    os.makedirs(args.gradio_temp_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "def gpu_wrapped_execute_video(*args, **kwargs):\n",
        "    return gradio_pipeline.execute_video(*args, **kwargs)\n",
        "\n",
        "\n",
        "def gpu_wrapped_execute_image_retargeting(*args, **kwargs):\n",
        "    return gradio_pipeline.execute_image_retargeting(*args, **kwargs)\n",
        "\n",
        "\n",
        "def gpu_wrapped_execute_video_retargeting(*args, **kwargs):\n",
        "    return gradio_pipeline.execute_video_retargeting(*args, **kwargs)\n",
        "\n",
        "\n",
        "def reset_sliders(*args, **kwargs):\n",
        "    return 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.5, True, True\n",
        "\n",
        "\n",
        "# assets\n",
        "title_md = \"assets/gradio/gradio_title.md\"\n",
        "example_portrait_dir = \"assets/examples/source\"\n",
        "example_video_dir = \"assets/examples/driving\"\n",
        "data_examples_i2v = [\n",
        "    [osp.join(example_portrait_dir, \"s9.jpg\"), osp.join(example_video_dir, \"d0.mp4\"), True, True, True, False],\n",
        "    [osp.join(example_portrait_dir, \"s6.jpg\"), osp.join(example_video_dir, \"d0.mp4\"), True, True, True, False],\n",
        "    [osp.join(example_portrait_dir, \"s10.jpg\"), osp.join(example_video_dir, \"d0.mp4\"), True, True, True, False],\n",
        "    [osp.join(example_portrait_dir, \"s5.jpg\"), osp.join(example_video_dir, \"d18.mp4\"), True, True, True, False],\n",
        "    [osp.join(example_portrait_dir, \"s7.jpg\"), osp.join(example_video_dir, \"d19.mp4\"), True, True, True, False],\n",
        "    [osp.join(example_portrait_dir, \"s2.jpg\"), osp.join(example_video_dir, \"d13.mp4\"), True, True, True, True],\n",
        "]\n",
        "data_examples_v2v = [\n",
        "    [osp.join(example_portrait_dir, \"s13.mp4\"), osp.join(example_video_dir, \"d0.mp4\"), True, True, True, False, 3e-7],\n",
        "    # [osp.join(example_portrait_dir, \"s14.mp4\"), osp.join(example_video_dir, \"d18.mp4\"), True, True, True, False, False, 3e-7],\n",
        "    # [osp.join(example_portrait_dir, \"s15.mp4\"), osp.join(example_video_dir, \"d19.mp4\"), True, True, True, False, False, 3e-7],\n",
        "    [osp.join(example_portrait_dir, \"s18.mp4\"), osp.join(example_video_dir, \"d6.mp4\"), True, True, True, False, 3e-7],\n",
        "    # [osp.join(example_portrait_dir, \"s19.mp4\"), osp.join(example_video_dir, \"d6.mp4\"), True, True, True, False, False, 3e-7],\n",
        "    [osp.join(example_portrait_dir, \"s20.mp4\"), osp.join(example_video_dir, \"d0.mp4\"), True, True, True, False, 3e-7],\n",
        "]\n",
        "#################### interface logic ####################\n",
        "\n",
        "# Define components first\n",
        "retargeting_source_scale = gr.Number(minimum=1.8, maximum=3.2, value=2.5, step=0.05, label=\"crop scale\")\n",
        "video_retargeting_source_scale = gr.Number(minimum=1.8, maximum=3.2, value=2.3, step=0.05, label=\"crop scale\")\n",
        "driving_smooth_observation_variance_retargeting = gr.Number(value=3e-6, label=\"motion smooth strength\", minimum=1e-11, maximum=1e-2, step=1e-8)\n",
        "video_retargeting_silence = gr.Checkbox(value=False, label=\"keeping the lip silent\")\n",
        "eye_retargeting_slider = gr.Slider(minimum=0, maximum=0.8, step=0.01, label=\"target eyes-open ratio\")\n",
        "lip_retargeting_slider = gr.Slider(minimum=0, maximum=0.8, step=0.01, label=\"target lip-open ratio\")\n",
        "video_lip_retargeting_slider = gr.Slider(minimum=0, maximum=0.8, step=0.01, label=\"target lip-open ratio\")\n",
        "head_pitch_slider = gr.Slider(minimum=-15.0, maximum=15.0, value=0, step=1, label=\"relative pitch\")\n",
        "head_yaw_slider = gr.Slider(minimum=-25, maximum=25, value=0, step=1, label=\"relative yaw\")\n",
        "head_roll_slider = gr.Slider(minimum=-15.0, maximum=15.0, value=0, step=1, label=\"relative roll\")\n",
        "mov_x = gr.Slider(minimum=-0.19, maximum=0.19, value=0.0, step=0.01, label=\"x-axis movement\")\n",
        "mov_y = gr.Slider(minimum=-0.19, maximum=0.19, value=0.0, step=0.01, label=\"y-axis movement\")\n",
        "mov_z = gr.Slider(minimum=0.9, maximum=1.2, value=1.0, step=0.01, label=\"z-axis movement\")\n",
        "lip_variation_zero = gr.Slider(minimum=-0.09, maximum=0.09, value=0, step=0.01, label=\"pouting\")\n",
        "lip_variation_one = gr.Slider(minimum=-20.0, maximum=15.0, value=0, step=0.01, label=\"pursing üòê\")\n",
        "lip_variation_two = gr.Slider(minimum=0.0, maximum=15.0, value=0, step=0.01, label=\"grin üòÅ\")\n",
        "lip_variation_three = gr.Slider(minimum=-90.0, maximum=120.0, value=0, step=1.0, label=\"lip close <-> open\")\n",
        "smile = gr.Slider(minimum=-0.3, maximum=1.3, value=0, step=0.01, label=\"smile üòÑ\")\n",
        "wink = gr.Slider(minimum=0, maximum=39, value=0, step=0.01, label=\"wink üòâ\")\n",
        "eyebrow = gr.Slider(minimum=-30, maximum=30, value=0, step=0.01, label=\"eyebrow ü§®\")\n",
        "eyeball_direction_x = gr.Slider(minimum=-30.0, maximum=30.0, value=0, step=0.01, label=\"eye gaze (horizontal) üëÄ\")\n",
        "eyeball_direction_y = gr.Slider(minimum=-63.0, maximum=63.0, value=0, step=0.01, label=\"eye gaze (vertical) üôÑ\")\n",
        "retargeting_input_image = gr.Image(type=\"filepath\")\n",
        "retargeting_input_video = gr.Video()\n",
        "output_image = gr.Image(type=\"numpy\")\n",
        "output_image_paste_back = gr.Image(type=\"numpy\")\n",
        "retargeting_output_image = gr.Image(type=\"numpy\")\n",
        "retargeting_output_image_paste_back = gr.Image(type=\"numpy\")\n",
        "output_video = gr.Video(autoplay=False)\n",
        "output_video_paste_back = gr.Video(autoplay=False)\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft(font=[gr.themes.GoogleFont(\"Plus Jakarta Sans\")])) as demo:\n",
        "    gr.HTML(load_description(title_md))\n",
        "\n",
        "    gr.Markdown(load_description(\"assets/gradio/gradio_description_upload.md\"))\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            with gr.Tabs():\n",
        "                with gr.TabItem(\"üñºÔ∏è Source Image\") as tab_image:\n",
        "                    with gr.Accordion(open=True, label=\"Source Image\"):\n",
        "                        source_image_input = gr.Image(type=\"filepath\")\n",
        "                        gr.Examples(\n",
        "                            examples=[\n",
        "                                [osp.join(example_portrait_dir, \"s9.jpg\")],\n",
        "                                [osp.join(example_portrait_dir, \"s6.jpg\")],\n",
        "                                [osp.join(example_portrait_dir, \"s10.jpg\")],\n",
        "                                [osp.join(example_portrait_dir, \"s5.jpg\")],\n",
        "                                [osp.join(example_portrait_dir, \"s7.jpg\")],\n",
        "                                [osp.join(example_portrait_dir, \"s12.jpg\")],\n",
        "                                [osp.join(example_portrait_dir, \"s22.jpg\")],\n",
        "                                [osp.join(example_portrait_dir, \"s23.jpg\")],\n",
        "                            ],\n",
        "                            inputs=[source_image_input],\n",
        "                            cache_examples=False,\n",
        "                        )\n",
        "\n",
        "                with gr.TabItem(\"üéûÔ∏è Source Video\") as tab_video:\n",
        "                    with gr.Accordion(open=True, label=\"Source Video\"):\n",
        "                        source_video_input = gr.Video()\n",
        "                        gr.Examples(\n",
        "                            examples=[\n",
        "                                [osp.join(example_portrait_dir, \"s13.mp4\")],\n",
        "                                # [osp.join(example_portrait_dir, \"s14.mp4\")],\n",
        "                                # [osp.join(example_portrait_dir, \"s15.mp4\")],\n",
        "                                [osp.join(example_portrait_dir, \"s18.mp4\")],\n",
        "                                # [osp.join(example_portrait_dir, \"s19.mp4\")],\n",
        "                                [osp.join(example_portrait_dir, \"s20.mp4\")],\n",
        "                            ],\n",
        "                            inputs=[source_video_input],\n",
        "                            cache_examples=False,\n",
        "                        )\n",
        "\n",
        "                tab_selection = gr.Textbox(visible=False)\n",
        "                tab_image.select(lambda: \"Image\", None, tab_selection)\n",
        "                tab_video.select(lambda: \"Video\", None, tab_selection)\n",
        "            with gr.Accordion(open=True, label=\"Cropping Options for Source Image or Video\"):\n",
        "                with gr.Row():\n",
        "                    flag_do_crop_input = gr.Checkbox(value=True, label=\"do crop (source)\")\n",
        "                    scale = gr.Number(value=2.3, label=\"source crop scale\", minimum=1.8, maximum=3.2, step=0.05)\n",
        "                    vx_ratio = gr.Number(value=0.0, label=\"source crop x\", minimum=-0.5, maximum=0.5, step=0.01)\n",
        "                    vy_ratio = gr.Number(value=-0.125, label=\"source crop y\", minimum=-0.5, maximum=0.5, step=0.01)\n",
        "\n",
        "        with gr.Column():\n",
        "            with gr.Tabs():\n",
        "                with gr.TabItem(\"üéûÔ∏è Driving Video\") as v_tab_video:\n",
        "                    with gr.Accordion(open=True, label=\"Driving Video\"):\n",
        "                        driving_video_input = gr.Video()\n",
        "                        gr.Examples(\n",
        "                            examples=[\n",
        "                                [osp.join(example_video_dir, \"d0.mp4\")],\n",
        "                                [osp.join(example_video_dir, \"d18.mp4\")],\n",
        "                                [osp.join(example_video_dir, \"d19.mp4\")],\n",
        "                                [osp.join(example_video_dir, \"d14.mp4\")],\n",
        "                                [osp.join(example_video_dir, \"d6.mp4\")],\n",
        "                                [osp.join(example_video_dir, \"d20.mp4\")],\n",
        "                            ],\n",
        "                            inputs=[driving_video_input],\n",
        "                            cache_examples=False,\n",
        "                        )\n",
        "                with gr.TabItem(\"üñºÔ∏è Driving Image\") as v_tab_image:\n",
        "                    with gr.Accordion(open=True, label=\"Driving Image\"):\n",
        "                        driving_image_input = gr.Image(type=\"filepath\")\n",
        "                        gr.Examples(\n",
        "                            examples=[\n",
        "                                [osp.join(example_video_dir, \"d30.jpg\")],\n",
        "                                [osp.join(example_video_dir, \"d9.jpg\")],\n",
        "                                [osp.join(example_video_dir, \"d19.jpg\")],\n",
        "                                [osp.join(example_video_dir, \"d8.jpg\")],\n",
        "                                [osp.join(example_video_dir, \"d12.jpg\")],\n",
        "                                [osp.join(example_video_dir, \"d38.jpg\")],\n",
        "                            ],\n",
        "                            inputs=[driving_image_input],\n",
        "                            cache_examples=False,\n",
        "                        )\n",
        "\n",
        "                with gr.TabItem(\"üìÅ Driving Pickle\") as v_tab_pickle:\n",
        "                    with gr.Accordion(open=True, label=\"Driving Pickle\"):\n",
        "                        driving_video_pickle_input = gr.File(type=\"filepath\", file_types=[\".pkl\"])\n",
        "                        gr.Examples(\n",
        "                            examples=[\n",
        "                                [osp.join(example_video_dir, \"d1.pkl\")],\n",
        "                                [osp.join(example_video_dir, \"d2.pkl\")],\n",
        "                                [osp.join(example_video_dir, \"d5.pkl\")],\n",
        "                                [osp.join(example_video_dir, \"d7.pkl\")],\n",
        "                                [osp.join(example_video_dir, \"d8.pkl\")],\n",
        "                            ],\n",
        "                            inputs=[driving_video_pickle_input],\n",
        "                            cache_examples=False,\n",
        "                        )\n",
        "\n",
        "                v_tab_selection = gr.Textbox(visible=False)\n",
        "                v_tab_video.select(lambda: \"Video\", None, v_tab_selection)\n",
        "                v_tab_image.select(lambda: \"Image\", None, v_tab_selection)\n",
        "                v_tab_pickle.select(lambda: \"Pickle\", None, v_tab_selection)\n",
        "            # with gr.Accordion(open=False, label=\"Animation Instructions\"):\n",
        "                # gr.Markdown(load_description(\"assets/gradio/gradio_description_animation.md\"))\n",
        "            with gr.Accordion(open=True, label=\"Cropping Options for Driving Video\"):\n",
        "                with gr.Row():\n",
        "                    flag_crop_driving_video_input = gr.Checkbox(value=False, label=\"do crop (driving)\")\n",
        "                    scale_crop_driving_video = gr.Number(value=2.2, label=\"driving crop scale\", minimum=1.8, maximum=3.2, step=0.05)\n",
        "                    vx_ratio_crop_driving_video = gr.Number(value=0.0, label=\"driving crop x\", minimum=-0.5, maximum=0.5, step=0.01)\n",
        "                    vy_ratio_crop_driving_video = gr.Number(value=-0.1, label=\"driving crop y\", minimum=-0.5, maximum=0.5, step=0.01)\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Accordion(open=True, label=\"Animation Options\"):\n",
        "            with gr.Row():\n",
        "                flag_normalize_lip = gr.Checkbox(value=False, label=\"normalize lip\")\n",
        "                flag_relative_input = gr.Checkbox(value=True, label=\"relative motion\")\n",
        "                flag_remap_input = gr.Checkbox(value=True, label=\"paste-back\")\n",
        "                flag_stitching_input = gr.Checkbox(value=True, label=\"stitching\")\n",
        "                animation_region = gr.Radio([\"exp\", \"pose\", \"lip\", \"eyes\", \"all\"], value=\"all\", label=\"animation region\")\n",
        "                driving_option_input = gr.Radio(['expression-friendly', 'pose-friendly'], value=\"expression-friendly\", label=\"driving option (i2v)\")\n",
        "                driving_multiplier = gr.Number(value=1.0, label=\"driving multiplier (i2v)\", minimum=0.0, maximum=2.0, step=0.02)\n",
        "                driving_smooth_observation_variance = gr.Number(value=3e-7, label=\"motion smooth strength (v2v)\", minimum=1e-11, maximum=1e-2, step=1e-8)\n",
        "\n",
        "    gr.Markdown(load_description(\"assets/gradio/gradio_description_animate_clear.md\"))\n",
        "    with gr.Row():\n",
        "        process_button_animation = gr.Button(\"üöÄ Animate\", variant=\"primary\")\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            output_video_i2v = gr.Video(autoplay=False, label=\"The animated video in the original image space\")\n",
        "        with gr.Column():\n",
        "            output_video_concat_i2v = gr.Video(autoplay=False, label=\"The animated video\")\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            output_image_i2i = gr.Image(type=\"numpy\", label=\"The animated image in the original image space\", visible=False)\n",
        "        with gr.Column():\n",
        "            output_image_concat_i2i = gr.Image(type=\"numpy\", label=\"The animated image\", visible=False)\n",
        "    with gr.Row():\n",
        "        process_button_reset = gr.ClearButton([source_image_input, source_video_input, driving_video_pickle_input, driving_video_input, driving_image_input, output_video_i2v, output_video_concat_i2v, output_image_i2i, output_image_concat_i2i], value=\"üßπ Clear\")\n",
        "\n",
        "    with gr.Row():\n",
        "        # Examples\n",
        "        gr.Markdown(\"## You could also choose the examples below by one click ‚¨áÔ∏è\")\n",
        "    with gr.Row():\n",
        "        with gr.Tabs():\n",
        "            with gr.TabItem(\"üñºÔ∏è Portrait Animation\"):\n",
        "                gr.Examples(\n",
        "                    examples=data_examples_i2v,\n",
        "                    fn=gpu_wrapped_execute_video,\n",
        "                    inputs=[\n",
        "                        source_image_input,\n",
        "                        driving_video_input,\n",
        "                        flag_relative_input,\n",
        "                        flag_do_crop_input,\n",
        "                        flag_remap_input,\n",
        "                        flag_crop_driving_video_input,\n",
        "                    ],\n",
        "                    outputs=[output_image, output_image_paste_back],\n",
        "                    examples_per_page=len(data_examples_i2v),\n",
        "                    cache_examples=False,\n",
        "                )\n",
        "            with gr.TabItem(\"üéûÔ∏è Portrait Video Editing\"):\n",
        "                gr.Examples(\n",
        "                    examples=data_examples_v2v,\n",
        "                    fn=gpu_wrapped_execute_video,\n",
        "                    inputs=[\n",
        "                        source_video_input,\n",
        "                        driving_video_input,\n",
        "                        flag_relative_input,\n",
        "                        flag_do_crop_input,\n",
        "                        flag_remap_input,\n",
        "                        flag_crop_driving_video_input,\n",
        "                        driving_smooth_observation_variance,\n",
        "                    ],\n",
        "                    outputs=[output_image, output_image_paste_back],\n",
        "                    examples_per_page=len(data_examples_v2v),\n",
        "                    cache_examples=False,\n",
        "                )\n",
        "\n",
        "    # Retargeting Image\n",
        "    gr.Markdown(load_description(\"assets/gradio/gradio_description_retargeting.md\"), visible=True)\n",
        "    with gr.Row(visible=True):\n",
        "        flag_do_crop_input_retargeting_image = gr.Checkbox(value=True, label=\"do crop (source)\")\n",
        "        flag_stitching_retargeting_input = gr.Checkbox(value=True, label=\"stitching\")\n",
        "        retargeting_source_scale.render()\n",
        "        eye_retargeting_slider.render()\n",
        "        lip_retargeting_slider.render()\n",
        "    with gr.Row(visible=True):\n",
        "        with gr.Column():\n",
        "            with gr.Accordion(open=True, label=\"Facial movement sliders\"):\n",
        "                with gr.Row(visible=True):\n",
        "                    head_pitch_slider.render()\n",
        "                    head_yaw_slider.render()\n",
        "                    head_roll_slider.render()\n",
        "                with gr.Row(visible=True):\n",
        "                    mov_x.render()\n",
        "                    mov_y.render()\n",
        "                    mov_z.render()\n",
        "        with gr.Column():\n",
        "            with gr.Accordion(open=True, label=\"Facial expression sliders\"):\n",
        "                with gr.Row(visible=True):\n",
        "                    lip_variation_zero.render()\n",
        "                    lip_variation_one.render()\n",
        "                    lip_variation_two.render()\n",
        "                with gr.Row(visible=True):\n",
        "                    lip_variation_three.render()\n",
        "                    smile.render()\n",
        "                    wink.render()\n",
        "                with gr.Row(visible=True):\n",
        "                    eyebrow.render()\n",
        "                    eyeball_direction_x.render()\n",
        "                    eyeball_direction_y.render()\n",
        "    with gr.Row(visible=True):\n",
        "        reset_button = gr.Button(\"üîÑ Reset\")\n",
        "        reset_button.click(\n",
        "            fn=reset_sliders,\n",
        "            inputs=None,\n",
        "            outputs=[\n",
        "                head_pitch_slider, head_yaw_slider, head_roll_slider, mov_x, mov_y, mov_z,\n",
        "                lip_variation_zero, lip_variation_one, lip_variation_two, lip_variation_three, smile, wink, eyebrow, eyeball_direction_x, eyeball_direction_y,\n",
        "                retargeting_source_scale, flag_stitching_retargeting_input, flag_do_crop_input_retargeting_image\n",
        "            ]\n",
        "        )\n",
        "    with gr.Row(visible=True):\n",
        "        with gr.Column():\n",
        "            with gr.Accordion(open=True, label=\"Retargeting Image Input\"):\n",
        "                retargeting_input_image.render()\n",
        "                gr.Examples(\n",
        "                    examples=[\n",
        "                        [osp.join(example_portrait_dir, \"s9.jpg\")],\n",
        "                        [osp.join(example_portrait_dir, \"s6.jpg\")],\n",
        "                        [osp.join(example_portrait_dir, \"s10.jpg\")],\n",
        "                        [osp.join(example_portrait_dir, \"s5.jpg\")],\n",
        "                        [osp.join(example_portrait_dir, \"s7.jpg\")],\n",
        "                        [osp.join(example_portrait_dir, \"s12.jpg\")],\n",
        "                        [osp.join(example_portrait_dir, \"s22.jpg\")],\n",
        "                        # [osp.join(example_portrait_dir, \"s23.jpg\")],\n",
        "                        [osp.join(example_portrait_dir, \"s42.jpg\")],\n",
        "                    ],\n",
        "                    inputs=[retargeting_input_image],\n",
        "                    cache_examples=False,\n",
        "                )\n",
        "        with gr.Column():\n",
        "            with gr.Accordion(open=True, label=\"Retargeting Result\"):\n",
        "                retargeting_output_image.render()\n",
        "        with gr.Column():\n",
        "            with gr.Accordion(open=True, label=\"Paste-back Result\"):\n",
        "                retargeting_output_image_paste_back.render()\n",
        "    with gr.Row(visible=True):\n",
        "        process_button_reset_retargeting = gr.ClearButton(\n",
        "            [\n",
        "                retargeting_input_image,\n",
        "                retargeting_output_image,\n",
        "                retargeting_output_image_paste_back,\n",
        "            ],\n",
        "            value=\"üßπ Clear\"\n",
        "        )\n",
        "\n",
        "    # Retargeting Video\n",
        "    gr.Markdown(load_description(\"assets/gradio/gradio_description_retargeting_video.md\"), visible=True)\n",
        "    with gr.Row(visible=True):\n",
        "        flag_do_crop_input_retargeting_video = gr.Checkbox(value=True, label=\"do crop (source)\")\n",
        "        video_retargeting_source_scale.render()\n",
        "        video_lip_retargeting_slider.render()\n",
        "        driving_smooth_observation_variance_retargeting.render()\n",
        "        video_retargeting_silence.render()\n",
        "    with gr.Row(visible=True):\n",
        "        process_button_retargeting_video = gr.Button(\"üöó Retargeting Video\", variant=\"primary\")\n",
        "    with gr.Row(visible=True):\n",
        "        with gr.Column():\n",
        "            with gr.Accordion(open=True, label=\"Retargeting Video Input\"):\n",
        "                retargeting_input_video.render()\n",
        "                gr.Examples(\n",
        "                    examples=[\n",
        "                        [osp.join(example_portrait_dir, \"s13.mp4\")],\n",
        "                        # [osp.join(example_portrait_dir, \"s18.mp4\")],\n",
        "                        # [osp.join(example_portrait_dir, \"s20.mp4\")],\n",
        "                        [osp.join(example_portrait_dir, \"s29.mp4\")],\n",
        "                        [osp.join(example_portrait_dir, \"s32.mp4\")],\n",
        "                        [osp.join(example_video_dir, \"d3.mp4\")],\n",
        "                    ],\n",
        "                    inputs=[retargeting_input_video],\n",
        "                    cache_examples=False,\n",
        "                )\n",
        "        with gr.Column():\n",
        "            with gr.Accordion(open=True, label=\"Retargeting Result\"):\n",
        "                output_video.render()\n",
        "        with gr.Column():\n",
        "            with gr.Accordion(open=True, label=\"Paste-back Result\"):\n",
        "                output_video_paste_back.render()\n",
        "    with gr.Row(visible=True):\n",
        "        process_button_reset_retargeting = gr.ClearButton(\n",
        "            [\n",
        "                video_lip_retargeting_slider,\n",
        "                retargeting_input_video,\n",
        "                output_video,\n",
        "                output_video_paste_back\n",
        "            ],\n",
        "            value=\"üßπ Clear\"\n",
        "        )\n",
        "\n",
        "    # binding functions for buttons\n",
        "    process_button_animation.click(\n",
        "        fn=gpu_wrapped_execute_video,\n",
        "        inputs=[\n",
        "            source_image_input,\n",
        "            source_video_input,\n",
        "            driving_video_input,\n",
        "            driving_image_input,\n",
        "            driving_video_pickle_input,\n",
        "            flag_normalize_lip,\n",
        "            flag_relative_input,\n",
        "            flag_do_crop_input,\n",
        "            flag_remap_input,\n",
        "            flag_stitching_input,\n",
        "            animation_region,\n",
        "            driving_option_input,\n",
        "            driving_multiplier,\n",
        "            flag_crop_driving_video_input,\n",
        "            scale,\n",
        "            vx_ratio,\n",
        "            vy_ratio,\n",
        "            scale_crop_driving_video,\n",
        "            vx_ratio_crop_driving_video,\n",
        "            vy_ratio_crop_driving_video,\n",
        "            driving_smooth_observation_variance,\n",
        "            tab_selection,\n",
        "            v_tab_selection,\n",
        "        ],\n",
        "        outputs=[output_video_i2v, output_video_i2v, output_video_concat_i2v, output_video_concat_i2v, output_image_i2i, output_image_i2i, output_image_concat_i2i, output_image_concat_i2i],\n",
        "        show_progress=True\n",
        "    )\n",
        "\n",
        "\n",
        "    retargeting_input_image.change(\n",
        "        fn=gradio_pipeline.init_retargeting_image,\n",
        "        inputs=[retargeting_source_scale, eye_retargeting_slider, lip_retargeting_slider, retargeting_input_image],\n",
        "        outputs=[eye_retargeting_slider, lip_retargeting_slider]\n",
        "    )\n",
        "\n",
        "    sliders = [eye_retargeting_slider, lip_retargeting_slider, head_pitch_slider, head_yaw_slider, head_roll_slider, mov_x, mov_y, mov_z, lip_variation_zero, lip_variation_one, lip_variation_two, lip_variation_three, smile, wink, eyebrow, eyeball_direction_x, eyeball_direction_y]\n",
        "    for slider in sliders:\n",
        "        # NOTE: gradio >= 4.0.0 may cause slow response\n",
        "        slider.change(\n",
        "            fn=gpu_wrapped_execute_image_retargeting,\n",
        "            inputs=[\n",
        "                eye_retargeting_slider, lip_retargeting_slider, head_pitch_slider, head_yaw_slider, head_roll_slider, mov_x, mov_y, mov_z,\n",
        "                lip_variation_zero, lip_variation_one, lip_variation_two, lip_variation_three, smile, wink, eyebrow, eyeball_direction_x, eyeball_direction_y,\n",
        "                retargeting_input_image, retargeting_source_scale, flag_stitching_retargeting_input, flag_do_crop_input_retargeting_image\n",
        "            ],\n",
        "            outputs=[retargeting_output_image, retargeting_output_image_paste_back],\n",
        "        )\n",
        "\n",
        "    process_button_retargeting_video.click(\n",
        "        fn=gpu_wrapped_execute_video_retargeting,\n",
        "        inputs=[video_lip_retargeting_slider, retargeting_input_video, video_retargeting_source_scale, driving_smooth_observation_variance_retargeting, video_retargeting_silence, flag_do_crop_input_retargeting_video],\n",
        "        outputs=[output_video, output_video_paste_back],\n",
        "        show_progress=True\n",
        "    )\n",
        "\n",
        "# demo.launch(\n",
        "#     server_port=args.server_port,\n",
        "#     share=args.share,\n",
        "#     server_name=args.server_name\n",
        "# )\n",
        "demo.launch(debug=True,share=True)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "deJJJx03Qlx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Animal\n",
        "base_path=\"/content\"\n",
        "%cd $base_path/LivePortrait\n",
        "\"\"\"\n",
        "The entrance of the gradio for animal\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import tyro\n",
        "import subprocess\n",
        "import gradio as gr\n",
        "import os.path as osp\n",
        "from src.utils.helper import load_description\n",
        "from src.gradio_pipeline import GradioPipelineAnimal\n",
        "from src.config.crop_config import CropConfig\n",
        "from src.config.argument_config import ArgumentConfig\n",
        "from src.config.inference_config import InferenceConfig\n",
        "\n",
        "\n",
        "def partial_fields(target_class, kwargs):\n",
        "    return target_class(**{k: v for k, v in kwargs.items() if hasattr(target_class, k)})\n",
        "\n",
        "\n",
        "def fast_check_ffmpeg():\n",
        "    try:\n",
        "        subprocess.run([\"ffmpeg\", \"-version\"], capture_output=True, check=True)\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "\n",
        "# # set tyro theme\n",
        "# tyro.extras.set_accent_color(\"bright_cyan\")\n",
        "# args = tyro.cli(ArgumentConfig)\n",
        "\n",
        "\n",
        "\n",
        "default_args = {\n",
        "    \"server_name\": \"0.0.0.0\",\n",
        "    \"server_port\": 7860,\n",
        "    \"share\": True,\n",
        "    # Add other default values as needed\n",
        "}\n",
        "\n",
        "# Create an args object with default values\n",
        "args = ArgumentConfig(**default_args)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ffmpeg_dir = os.path.join(os.getcwd(), \"ffmpeg\")\n",
        "if osp.exists(ffmpeg_dir):\n",
        "    os.environ[\"PATH\"] += (os.pathsep + ffmpeg_dir)\n",
        "\n",
        "if not fast_check_ffmpeg():\n",
        "    raise ImportError(\n",
        "        \"FFmpeg is not installed. Please install FFmpeg (including ffmpeg and ffprobe) before running this script. https://ffmpeg.org/download.html\"\n",
        "    )\n",
        "# specify configs for inference\n",
        "inference_cfg = partial_fields(InferenceConfig, args.__dict__)  # use attribute of args to initial InferenceConfig\n",
        "crop_cfg = partial_fields(CropConfig, args.__dict__)  # use attribute of args to initial CropConfig\n",
        "\n",
        "gradio_pipeline_animal: GradioPipelineAnimal = GradioPipelineAnimal(\n",
        "    inference_cfg=inference_cfg,\n",
        "    crop_cfg=crop_cfg,\n",
        "    args=args\n",
        ")\n",
        "\n",
        "if args.gradio_temp_dir not in (None, ''):\n",
        "    os.environ[\"GRADIO_TEMP_DIR\"] = args.gradio_temp_dir\n",
        "    os.makedirs(args.gradio_temp_dir, exist_ok=True)\n",
        "\n",
        "def gpu_wrapped_execute_video(*args, **kwargs):\n",
        "    return gradio_pipeline_animal.execute_video(*args, **kwargs)\n",
        "\n",
        "\n",
        "# assets\n",
        "title_md = \"assets/gradio/gradio_title.md\"\n",
        "example_portrait_dir = \"assets/examples/source\"\n",
        "example_video_dir = \"assets/examples/driving\"\n",
        "data_examples_i2v = [\n",
        "    [osp.join(example_portrait_dir, \"s41.jpg\"), osp.join(example_video_dir, \"d3.mp4\"), True, False, False, False],\n",
        "    [osp.join(example_portrait_dir, \"s40.jpg\"), osp.join(example_video_dir, \"d6.mp4\"), True, False, False, False],\n",
        "    [osp.join(example_portrait_dir, \"s25.jpg\"), osp.join(example_video_dir, \"d19.mp4\"), True, False, False, False],\n",
        "]\n",
        "data_examples_i2v_pickle = [\n",
        "    [osp.join(example_portrait_dir, \"s25.jpg\"), osp.join(example_video_dir, \"wink.pkl\"), True, False, False, False],\n",
        "    [osp.join(example_portrait_dir, \"s40.jpg\"), osp.join(example_video_dir, \"talking.pkl\"), True, False, False, False],\n",
        "    [osp.join(example_portrait_dir, \"s41.jpg\"), osp.join(example_video_dir, \"aggrieved.pkl\"), True, False, False, False],\n",
        "]\n",
        "#################### interface logic ####################\n",
        "\n",
        "# Define components first\n",
        "output_image = gr.Image(type=\"numpy\")\n",
        "output_image_paste_back = gr.Image(type=\"numpy\")\n",
        "output_video_i2v = gr.Video(autoplay=False)\n",
        "output_video_concat_i2v = gr.Video(autoplay=False)\n",
        "output_video_i2v_gif = gr.Image(type=\"numpy\")\n",
        "\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft(font=[gr.themes.GoogleFont(\"Plus Jakarta Sans\")])) as demo:\n",
        "    gr.HTML(load_description(title_md))\n",
        "\n",
        "    gr.Markdown(load_description(\"assets/gradio/gradio_description_upload_animal.md\"))\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            with gr.Accordion(open=True, label=\"üê± Source Animal Image\"):\n",
        "                source_image_input = gr.Image(type=\"filepath\")\n",
        "                gr.Examples(\n",
        "                    examples=[\n",
        "                        [osp.join(example_portrait_dir, \"s25.jpg\")],\n",
        "                        [osp.join(example_portrait_dir, \"s30.jpg\")],\n",
        "                        [osp.join(example_portrait_dir, \"s31.jpg\")],\n",
        "                        [osp.join(example_portrait_dir, \"s32.jpg\")],\n",
        "                        [osp.join(example_portrait_dir, \"s39.jpg\")],\n",
        "                        [osp.join(example_portrait_dir, \"s40.jpg\")],\n",
        "                        [osp.join(example_portrait_dir, \"s41.jpg\")],\n",
        "                        [osp.join(example_portrait_dir, \"s38.jpg\")],\n",
        "                        [osp.join(example_portrait_dir, \"s36.jpg\")],\n",
        "                    ],\n",
        "                    inputs=[source_image_input],\n",
        "                    cache_examples=False,\n",
        "                )\n",
        "\n",
        "            with gr.Accordion(open=True, label=\"Cropping Options for Source Image\"):\n",
        "                with gr.Row():\n",
        "                    flag_do_crop_input = gr.Checkbox(value=True, label=\"do crop (source)\")\n",
        "                    scale = gr.Number(value=2.3, label=\"source crop scale\", minimum=1.8, maximum=3.2, step=0.05)\n",
        "                    vx_ratio = gr.Number(value=0.0, label=\"source crop x\", minimum=-0.5, maximum=0.5, step=0.01)\n",
        "                    vy_ratio = gr.Number(value=-0.125, label=\"source crop y\", minimum=-0.5, maximum=0.5, step=0.01)\n",
        "\n",
        "        with gr.Column():\n",
        "            with gr.Tabs():\n",
        "                with gr.TabItem(\"üìÅ Driving Pickle\") as tab_pickle:\n",
        "                    with gr.Accordion(open=True, label=\"Driving Pickle\"):\n",
        "                        driving_video_pickle_input = gr.File()\n",
        "                        gr.Examples(\n",
        "                            examples=[\n",
        "                                [osp.join(example_video_dir, \"wink.pkl\")],\n",
        "                                [osp.join(example_video_dir, \"shy.pkl\")],\n",
        "                                [osp.join(example_video_dir, \"aggrieved.pkl\")],\n",
        "                                [osp.join(example_video_dir, \"open_lip.pkl\")],\n",
        "                                [osp.join(example_video_dir, \"laugh.pkl\")],\n",
        "                                [osp.join(example_video_dir, \"talking.pkl\")],\n",
        "                                [osp.join(example_video_dir, \"shake_face.pkl\")],\n",
        "                            ],\n",
        "                            inputs=[driving_video_pickle_input],\n",
        "                            cache_examples=False,\n",
        "                        )\n",
        "                with gr.TabItem(\"üéûÔ∏è Driving Video\") as tab_video:\n",
        "                    with gr.Accordion(open=True, label=\"Driving Video\"):\n",
        "                        driving_video_input = gr.Video()\n",
        "                        gr.Examples(\n",
        "                            examples=[\n",
        "                                # [osp.join(example_video_dir, \"d0.mp4\")],\n",
        "                                # [osp.join(example_video_dir, \"d18.mp4\")],\n",
        "                                [osp.join(example_video_dir, \"d19.mp4\")],\n",
        "                                [osp.join(example_video_dir, \"d14.mp4\")],\n",
        "                                [osp.join(example_video_dir, \"d6.mp4\")],\n",
        "                                [osp.join(example_video_dir, \"d3.mp4\")],\n",
        "                            ],\n",
        "                            inputs=[driving_video_input],\n",
        "                            cache_examples=False,\n",
        "                        )\n",
        "\n",
        "                    tab_selection = gr.Textbox(visible=False)\n",
        "                    tab_pickle.select(lambda: \"Pickle\", None, tab_selection)\n",
        "                    tab_video.select(lambda: \"Video\", None, tab_selection)\n",
        "            with gr.Accordion(open=True, label=\"Cropping Options for Driving Video\"):\n",
        "                with gr.Row():\n",
        "                    flag_crop_driving_video_input = gr.Checkbox(value=False, label=\"do crop (driving)\")\n",
        "                    scale_crop_driving_video = gr.Number(value=2.2, label=\"driving crop scale\", minimum=1.8, maximum=3.2, step=0.05)\n",
        "                    vx_ratio_crop_driving_video = gr.Number(value=0.0, label=\"driving crop x\", minimum=-0.5, maximum=0.5, step=0.01)\n",
        "                    vy_ratio_crop_driving_video = gr.Number(value=-0.1, label=\"driving crop y\", minimum=-0.5, maximum=0.5, step=0.01)\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Accordion(open=False, label=\"Animation Options\"):\n",
        "            with gr.Row():\n",
        "                flag_stitching = gr.Checkbox(value=False, label=\"stitching (not recommended)\")\n",
        "                flag_remap_input = gr.Checkbox(value=False, label=\"paste-back (not recommended)\")\n",
        "                driving_multiplier = gr.Number(value=1.0, label=\"driving multiplier\", minimum=0.0, maximum=2.0, step=0.02)\n",
        "\n",
        "    gr.Markdown(load_description(\"assets/gradio/gradio_description_animate_clear.md\"))\n",
        "    with gr.Row():\n",
        "        process_button_animation = gr.Button(\"üöÄ Animate\", variant=\"primary\")\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            with gr.Accordion(open=True, label=\"The animated video in the cropped image space\"):\n",
        "                output_video_i2v.render()\n",
        "        with gr.Column():\n",
        "            with gr.Accordion(open=True, label=\"The animated gif in the cropped image space\"):\n",
        "                output_video_i2v_gif.render()\n",
        "        with gr.Column():\n",
        "            with gr.Accordion(open=True, label=\"The animated video\"):\n",
        "                output_video_concat_i2v.render()\n",
        "    with gr.Row():\n",
        "        process_button_reset = gr.ClearButton([source_image_input, driving_video_input, output_video_i2v, output_video_concat_i2v, output_video_i2v_gif], value=\"üßπ Clear\")\n",
        "\n",
        "    with gr.Row():\n",
        "        # Examples\n",
        "        gr.Markdown(\"## You could also choose the examples below by one click ‚¨áÔ∏è\")\n",
        "    with gr.Row():\n",
        "        with gr.Tabs():\n",
        "            with gr.TabItem(\"üìÅ Driving Pickle\") as tab_video:\n",
        "                gr.Examples(\n",
        "                    examples=data_examples_i2v_pickle,\n",
        "                    fn=gpu_wrapped_execute_video,\n",
        "                    inputs=[\n",
        "                        source_image_input,\n",
        "                        driving_video_pickle_input,\n",
        "                        flag_do_crop_input,\n",
        "                        flag_stitching,\n",
        "                        flag_remap_input,\n",
        "                        flag_crop_driving_video_input,\n",
        "                    ],\n",
        "                    outputs=[output_image, output_image_paste_back, output_video_i2v_gif],\n",
        "                    examples_per_page=len(data_examples_i2v_pickle),\n",
        "                    cache_examples=False,\n",
        "                )\n",
        "            with gr.TabItem(\"üéûÔ∏è Driving Video\") as tab_video:\n",
        "                gr.Examples(\n",
        "                    examples=data_examples_i2v,\n",
        "                    fn=gpu_wrapped_execute_video,\n",
        "                    inputs=[\n",
        "                        source_image_input,\n",
        "                        driving_video_input,\n",
        "                        flag_do_crop_input,\n",
        "                        flag_stitching,\n",
        "                        flag_remap_input,\n",
        "                        flag_crop_driving_video_input,\n",
        "                    ],\n",
        "                    outputs=[output_image, output_image_paste_back, output_video_i2v_gif],\n",
        "                    examples_per_page=len(data_examples_i2v),\n",
        "                    cache_examples=False,\n",
        "                )\n",
        "\n",
        "    process_button_animation.click(\n",
        "        fn=gpu_wrapped_execute_video,\n",
        "        inputs=[\n",
        "            source_image_input,\n",
        "            driving_video_input,\n",
        "            driving_video_pickle_input,\n",
        "            flag_do_crop_input,\n",
        "            flag_remap_input,\n",
        "            driving_multiplier,\n",
        "            flag_stitching,\n",
        "            flag_crop_driving_video_input,\n",
        "            scale,\n",
        "            vx_ratio,\n",
        "            vy_ratio,\n",
        "            scale_crop_driving_video,\n",
        "            vx_ratio_crop_driving_video,\n",
        "            vy_ratio_crop_driving_video,\n",
        "            tab_selection,\n",
        "        ],\n",
        "        outputs=[output_video_i2v, output_video_concat_i2v, output_video_i2v_gif],\n",
        "        show_progress=True\n",
        "    )\n",
        "\n",
        "demo.launch(\n",
        "    share=True,\n",
        "    debug=True\n",
        ")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "sn_lN052W8QX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}